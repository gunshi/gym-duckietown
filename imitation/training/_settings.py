import math
import numpy as np
import tensorflow as tf
import pyglet

from gym_duckietown.envs import DuckietownEnv, DuckiebotEnv
from imitation.algorithms import SupervisedLearning
from imitation.training._logger import IILTrainingLogger
from imitation.learners import UANeuralNetworkPolicy, NeuralNetworkPolicy
from imitation.teachers import UAPurePursuitPolicy
from imitation.learners.parametrizations.tf import MonteCarloDropoutResnetOneRegression, MonteCarloDropoutResnetOneMixture

SEED = 19048  # generated by Google Random Generator (1 - 50,000)
DEBUG = False

MAP_NAME = 'udem1'
MAP_STARTING_POSES = [
    [[0.8, 0.0, 1.5], 10.90],
    [[0.8, 0.0, 2.5], 10.90],
    [[1.5, 0.0, 3.5], 12.56],
    [[2.5, 0.0, 3.5], 12.56],
    [[4.1, 0.0, 2.0], 14.14],
    [[2.8, 0.0, 0.8], 15.71],
]

# all with Dataset Aggregation
algorithms = ['supervised', 'dagger', 'aggrevate', 'safe_dagger', 'upms', 'upms-ne', 'upms-sl', 'upms-ne-sl']

# learner's parametrization
parametrization_names = ['resnet_one_regression', 'resnet_one_mixture']
parametrization_classes = [MonteCarloDropoutResnetOneRegression, MonteCarloDropoutResnetOneMixture]

# teacher
teacher_name = 'pure_pursuit'

# optimization
learning_rate = 1e-3
weight_decay = 1e-4
optimization_methods_names = ['adamw', 'adam', 'adagrad', 'rmsprop', 'sgd']
optimization_methods_classes = [
    tf.contrib.opt.AdamWOptimizer(weight_decay, learning_rate),
    tf.train.AdamOptimizer(learning_rate=learning_rate),
    tf.train.AdagradOptimizer(learning_rate=learning_rate),
    tf.train.RMSPropOptimizer(learning_rate=learning_rate),
    tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
]


# Task Configuration
HORIZONS = [128, 256, 512, 1024, 2048]
EPISODES = [64, 32, 16, 8, 4]

ITERATIONS = 4  # to 4

SUBMISSION_DIRECTORY = 'icra2019/'

# seeding
np.random.seed(SEED)
tf.set_random_seed(SEED)


def experimental_entry(algorithm, experiment_iteration, selected_parametrization, selected_horizon, selected_episode,
                       selected_optimization):
    return '{}/{}/{}/h{}e{}/{}_{}/{}_lr_{}/'.format(
        SUBMISSION_DIRECTORY,
        algorithm,
        experiment_iteration,
        HORIZONS[selected_horizon],
        EPISODES[selected_episode],
        teacher_name,
        parametrization_names[selected_parametrization],
        optimization_methods_names[selected_optimization],
        learning_rate
    )


def simulation(at):
    environment = DuckietownEnv(
        domain_rand=False,
        max_steps=math.inf,
        map_name=MAP_NAME
    )
    environment.reset()

    environment.cur_pos = np.array(at[0])
    environment.cur_angle = at[1]

    return environment


def robot():
    return DuckiebotEnv()


def teacher(env):
    return UAPurePursuitPolicy(
        env=env,
        following_distance=0.3,
        refresh_rate=1 / 30
    )


def optimization_method(index):
    return optimization_methods_classes[index]


def parametrization(iteration, optimization):
    return parametrization_classes[iteration](
        optimization_method(optimization)
    )




